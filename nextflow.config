process {
  executor = 'slurm'

  // Standard partition
  // - Max CPUs: 64
  // - Max memory per CPU: 8 GB
  // - Max walltime: 4 days
  // - Use for general-purpose jobs
  withLabel: standard_partition {
    queue = 'standard'
    cpus = 2           // Choose between 1 and 64
    memory = '4 GB'    // Can be any value <= CPUs * 8 GB
    time = '4h'        // Up to 4-00:00:00
  }

  // Longrun partition
  // - Max CPUs: 64
  // - Max memory per CPU: 8 GB
  // - Max walltime: 14 days
  // - Use for long-running jobs
  withLabel: longrun_partition {
    queue = 'longrun'
    cpus = 4           // Choose between 1 and 64
    memory = '6 GB'    // Can be any value <= CPUs * 8 GB
    time = '120h'      // Up to 14-00:00:00
  }

  // High energy job on standard partition
  // - For CPU-parallel tasks
  // - Ensure memory stays within per-CPU max
  withLabel: std_high_en_partition {
    queue = 'standard'
    cpus = 24          // Max 64
    memory = '48 GB'   // Can be any value <= CPUs * 8 GB
    time = '4h'
  }

  // Large memory jobs
  // - Runs on large_memory partition
  // - Max CPUs: 96
  // - Max memory per CPU: 64 GB
  // - Max walltime: 14 days
  // - Use for memory-heavy tools like Canu, SPAdes
  withLabel: large_memory_partition {
    queue = 'large_memory'
    cpus = 8            // Choose between 1 and 96
    memory = '16 GB'    // Can go high (e.g., 256 GB), must be <= CPUs * 64 GB
    time = '4h'
  }

  // GPU jobs
  // - Uses nodes with GPUs
  // - Max CPUs per node: 48
  // - Max walltime: 14 days
  // - Add/change '--gres=gpu:N' to request more GPUs
  withLabel: gpu_partition {
    queue = 'gpu'
    cpus = 8           // Choose between 1 and 48
    memory = '32 GB'   // Can be any value <= CPUs * 8 GB
    time = '4h'
    clusterOptions = '--gres=gpu:1'  // Request 1 GPU (e.g., A100, L40s, etc.)
  }
}

